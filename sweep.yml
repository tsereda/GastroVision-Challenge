program: train.py
method: bayes
metric:
  name: val/worst_
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 15
  eta: 3
  s: 2

parameters:
  model_name:
    value: 'swin_base_patch4_window7_224'
  
  image_size:
    value: 224
  
  # OPTIMIZED: Top runs cluster at lower values (8e-5 to 1e-4)
  # Extended lower bound to explore even smaller rates
  learning_rate:
    distribution: log_uniform_values
    min: 0.00004
    max: 0.00012
  
  # OPTIMIZED: Top runs favor higher values (0.0003-0.0004)
  weight_decay:
    values: [0.0003, 0.0004]
  
  batch_size:
    value: 32
  
  # INCREASED: More epochs for convergence
  epochs:
    value: 50
  
  # OPTIMIZED: Focus on top-performing value
  warmup_epochs:
    value: 12
  
  scheduler:
    value: 'cosine'
  
  loss_function:
    value: 'focal_weighted'
  
  # OPTIMIZED: Strong negative correlation - focus on low values
  # Top runs heavily cluster around 1.0
  focal_gamma:
    values: [0.8, 1.0, 1.2]
  
  # OPTIMIZED: Top runs favor 0.10-0.12
  label_smoothing:
    values: [0.10, 0.12]
  
  # OPTIMIZED: Top runs show tight cluster around 0.05-0.15
  mixup_alpha:
    min: 0.05
    max: 0.20
  
  # OPTIMIZED: Positive correlation, focus on higher values
  cutmix_alpha:
    min: 0.8
    max: 1.2
  
  # OPTIMIZED: Top runs split between 0.4 and 0.5
  mixup_prob:
    values: [0.4, 0.5]
  
  # Standard color augmentation - keep fixed
  color_jitter_brightness:
    value: 0.2
  
  color_jitter_contrast:
    value: 0.2
  
  color_jitter_saturation:
    value: 0.2
  
  # OPTIMIZED: Strong positive correlation - focus on high dropout
  dropout_rate:
    values: [0.30, 0.35]
  
  # OPTIMIZED: Top runs almost exclusively use 0.2
  stochastic_depth:
    value: 0.20
  
  # CRITICAL: Most important parameter - top runs cluster at 3.0-4.0
  # Extended upper range to explore even higher boosts
  class_weight_boost:
    min: 3.5
    max: 5.5